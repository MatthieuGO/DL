{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import *\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.nn.functional import interpolate, avg_pool2d\n",
    "import numpy as np\n",
    "from random import random\n",
    "from typing import Any, Dict, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss(tab_of_loss, path=\"./\"):\n",
    "    with open(path+\"tab_of_loss\", \"wb\") as floss:\n",
    "        pickle.dump(tab_of_loss, floss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'real_batch = next(iter(dataloader))\\nplt.figure(figsize=(8,8))\\nplt.axis(\"off\")\\nplt.title(\"Training Images\")\\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(\"cpu\")[:64], padding=2, normalize=True).cpu(),(1,2,0)))'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = \"/home/goliot/project/deep_learning_project/img_align_celeba\"\n",
    "\n",
    "batch_size = 20\n",
    "image_size = 128\n",
    "dataset = datasets.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "\n",
    "\"\"\"real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(\"cpu\")[:64], padding=2, normalize=True).cpu(),(1,2,0)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "def nf(\n",
    "    stage: int,\n",
    "    fmap_base: int = 16 << 10,\n",
    "    fmap_decay: float = 1.0,\n",
    "    fmap_min: int = 1,\n",
    "    fmap_max: int = 512,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    computes the number of fmaps present in each stage\n",
    "    Args:\n",
    "        stage: stage level\n",
    "        fmap_base: base number of fmaps\n",
    "        fmap_decay: decay rate for the fmaps in the network\n",
    "        fmap_min: minimum number of fmaps\n",
    "        fmap_max: maximum number of fmaps\n",
    "    Returns: number of fmaps that should be present there\n",
    "    \"\"\"\n",
    "    return int(\n",
    "        np.clip(\n",
    "            int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_min, fmap_max,\n",
    "        ).item()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel Wise Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelwiseNorm(Module):\n",
    "    \"\"\"\n",
    "    ------------------------------------------------------------------------------------\n",
    "    Pixelwise feature vector normalization.\n",
    "    reference:\n",
    "    https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py#L120\n",
    "    ------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PixelwiseNorm, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x: Tensor, alpha: float = 1e-8) -> Tensor:\n",
    "        y = x.pow(2.0).mean(dim=1, keepdim=True).add(alpha).sqrt()  # [N1HW]\n",
    "        y = x / y  # normalize the input x volume\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalized Conv Transpose 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedConvTranspose2d(ConvTranspose2d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        output_padding=0,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        dilation=1,\n",
    "        padding_mode=\"zeros\",\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            output_padding,\n",
    "            groups,\n",
    "            bias,\n",
    "            dilation,\n",
    "            padding_mode,\n",
    "        )\n",
    "        # make sure that the self.weight and self.bias are initialized according to\n",
    "        # random normal distribution\n",
    "        torch.nn.init.normal_(self.weight)\n",
    "        if bias:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "        # define the scale for the weights:\n",
    "        fan_in = self.in_channels\n",
    "        self.scale = np.sqrt(2) / np.sqrt(fan_in)\n",
    "\n",
    "    def forward(self, x: Tensor, output_size: Any = None) -> Tensor:\n",
    "        output_padding = self._output_padding(\n",
    "            input, output_size, self.stride, self.padding, self.kernel_size\n",
    "        )\n",
    "        return torch.conv_transpose2d(\n",
    "            input=x,\n",
    "            weight=self.weight * self.scale,  # scale the weight on runtime\n",
    "            bias=self.bias,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            output_padding=output_padding,\n",
    "            groups=self.groups,\n",
    "            dilation=self.dilation,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalized Conv 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EqualizedConv2d(Conv2d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        padding_mode=\"zeros\",\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            dilation,\n",
    "            groups,\n",
    "            bias,\n",
    "            padding_mode,\n",
    "        )\n",
    "        # make sure that the self.weight and self.bias are initialized according to\n",
    "        # random normal distribution\n",
    "        torch.nn.init.normal_(self.weight)\n",
    "        if bias:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "        # define the scale for the weights:\n",
    "        fan_in = np.prod(self.kernel_size) * self.in_channels\n",
    "        self.scale = np.sqrt(2) / np.sqrt(fan_in)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return torch.conv2d(\n",
    "            input=x,\n",
    "            weight=self.weight * self.scale,  # scale the weight on runtime\n",
    "            bias=self.bias,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            dilation=self.dilation,\n",
    "            groups=self.groups,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Initial Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenInitialBlock(Module):\n",
    "    \"\"\"\n",
    "    Module implementing the initial block of the input\n",
    "    Args:\n",
    "        in_channels: number of input channels to the block\n",
    "        out_channels: number of output channels of the block\n",
    "        use_eql: whether to use equalized learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, use_eql: bool) -> None:\n",
    "        super(GenInitialBlock, self).__init__()\n",
    "        self.use_eql = use_eql\n",
    "\n",
    "        ConvBlock = EqualizedConv2d if use_eql else Conv2d\n",
    "        ConvTransposeBlock = EqualizedConvTranspose2d if use_eql else ConvTranspose2d\n",
    "\n",
    "        self.conv_1 = ConvTransposeBlock(in_channels, out_channels, (4, 4), bias=True)\n",
    "        self.conv_2 = ConvBlock(\n",
    "            out_channels, out_channels, (3, 3), padding=1, bias=True\n",
    "        )\n",
    "        self.pixNorm = PixelwiseNorm()\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        y = torch.unsqueeze(torch.unsqueeze(x, -1), -1)\n",
    "        y = self.pixNorm(y)  # normalize the latents to hypersphere\n",
    "        y = self.lrelu(self.conv_1(y))\n",
    "        y = self.lrelu(self.conv_2(y))\n",
    "        y = self.pixNorm(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genrator General Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenGeneralConvBlock(Module):\n",
    "    \"\"\"\n",
    "    Module implementing a general convolutional block\n",
    "    Args:\n",
    "        in_channels: number of input channels to the block\n",
    "        out_channels: number of output channels required\n",
    "        use_eql: whether to use equalized learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, use_eql: bool) -> None:\n",
    "        super(GenGeneralConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels\n",
    "        self.use_eql = use_eql\n",
    "\n",
    "        ConvBlock = EqualizedConv2d if use_eql else Conv2d\n",
    "\n",
    "        self.conv_1 = ConvBlock(in_channels, out_channels, (3, 3), padding=1, bias=True)\n",
    "        self.conv_2 = ConvBlock(\n",
    "            out_channels, out_channels, (3, 3), padding=1, bias=True\n",
    "        )\n",
    "        self.pixNorm = PixelwiseNorm()\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        y = interpolate(x, scale_factor=2)\n",
    "        y = self.pixNorm(self.lrelu(self.conv_1(y)))\n",
    "        y = self.pixNorm(self.lrelu(self.conv_2(y)))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Final block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisFinalBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Final block for the Discriminator\n",
    "    Args:\n",
    "        in_channels: number of input channels\n",
    "        use_eql: whether to use equalized learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, use_eql: bool) -> None:\n",
    "        super(DisFinalBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_eql = use_eql\n",
    "\n",
    "        ConvBlock = EqualizedConv2d if use_eql else Conv2d\n",
    "\n",
    "        self.conv_1 = ConvBlock(\n",
    "            in_channels + 1, in_channels, (3, 3), padding=1, bias=True\n",
    "        )\n",
    "        # (16 - 3 + 2 / 1) + 1 = 16\n",
    "        self.conv_2 = ConvBlock(in_channels, out_channels, (4, 4), bias=True)\n",
    "        # (16 - 4 / 1) + 1 = 13\n",
    "        self.conv_3 = ConvBlock(out_channels, 1, (1, 1), bias=True)\n",
    "        self.batch_discriminator = MinibatchStdDev()\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        y = self.batch_discriminator(x)\n",
    "        y = self.lrelu(self.conv_1(y))\n",
    "        y = self.lrelu(self.conv_2(y))\n",
    "        y = self.conv_3(y)\n",
    "        return y.view(20, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator General Conv Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisGeneralConvBlock(Module):\n",
    "    \"\"\"\n",
    "    General block in the discriminator\n",
    "    Args:\n",
    "        in_channels: number of input channels\n",
    "        out_channels: number of output channels\n",
    "        use_eql: whether to use equalized learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, use_eql: bool) -> None:\n",
    "        super(DisGeneralConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_eql = use_eql\n",
    "\n",
    "        ConvBlock = EqualizedConv2d if use_eql else Conv2d\n",
    "\n",
    "        self.conv_1 = ConvBlock(in_channels, in_channels, (3, 3), padding=1, bias=True)\n",
    "        self.conv_2 = ConvBlock(in_channels, out_channels, (3, 3), padding=1, bias=True)\n",
    "        self.downSampler = AvgPool2d(2)\n",
    "        self.lrelu = LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        y = self.lrelu(self.conv_1(x))\n",
    "        y = self.lrelu(self.conv_2(y))\n",
    "        y = self.downSampler(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Standart deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchStdDev(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Minibatch standard deviation layer for the discriminator\n",
    "    Args:\n",
    "        group_size: Size of each group into which the batch is split\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, group_size: int = 4) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            group_size: Size of each group into which the batch is split\n",
    "        \"\"\"\n",
    "        super(MinibatchStdDev, self).__init__()\n",
    "        self.group_size = group_size\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"group_size={self.group_size}\"\n",
    "\n",
    "    def forward(self, x: Tensor, alpha: float = 1e-8) -> Tensor:\n",
    "        \"\"\"\n",
    "        forward pass of the layer\n",
    "        Args:\n",
    "            x: input activation volume\n",
    "            alpha: small number for numerical stability\n",
    "        Returns: y => x appended with standard deviation constant map\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        if batch_size > self.group_size:\n",
    "            assert batch_size % self.group_size == 0, (\n",
    "                f\"batch_size {batch_size} should be \"\n",
    "                f\"perfectly divisible by group_size {self.group_size}\"\n",
    "            )\n",
    "            group_size = self.group_size\n",
    "        else:\n",
    "            group_size = batch_size\n",
    "\n",
    "        # reshape x into a more amenable sized tensor\n",
    "        y = torch.reshape(x, [group_size, -1, channels, height, width])\n",
    "\n",
    "        # indicated shapes are after performing the operation\n",
    "        # [G x M x C x H x W] Subtract mean over groups\n",
    "        y = y - y.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # [M x C x H x W] Calc standard deviation over the groups\n",
    "        y = torch.sqrt(y.square().mean(dim=0, keepdim=False) + alpha)\n",
    "\n",
    "        # [M x 1 x 1 x 1]  Take average over feature_maps and pixels.\n",
    "        y = y.mean(dim=[1, 2, 3], keepdim=True)\n",
    "\n",
    "        # [B x 1 x H x W]  Replicate over group and pixels\n",
    "        y = y.repeat(group_size, 1, height, width)\n",
    "\n",
    "        # [B x (C + 1) x H x W]  Append as new feature_map.\n",
    "        y = torch.cat([x, y], 1)\n",
    "\n",
    "        # return the computed values:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(x, i, n):\n",
    "    hw = x.shape[3] + 1\n",
    "    assert ((hw - 1) - n) % i == 0 and (hw - 1) > n, \"Error\"\n",
    "    new_img = []\n",
    "    x = x.numpy()\n",
    "    for image in x:\n",
    "        for line in range(0, hw - n, 1):\n",
    "            for stride in range(0, hw - n, i):\n",
    "                new_img.append(image[:, line:line + n, stride:stride + n])\n",
    "    new_img = np.array(new_img)\n",
    "    return torch.from_numpy(new_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Module):\n",
    "    def __init__(self, in_channels, out_channels, use_eql):\n",
    "        super().__init__()\n",
    "\n",
    "        ConvBlock = EqualizedConv2d if use_eql else Conv2d\n",
    "        self.gen_block = ModuleList([GenGeneralConvBlock(nf(8), nf(9), use_eql)])\n",
    "\n",
    "        self.rgb_converters = ModuleList(\n",
    "            [\n",
    "                ConvBlock(nf(9), 3, kernel_size=(1, 1))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.gen_block:\n",
    "            x = layer(x)\n",
    "        x = self.rgb_converters[0](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def addGenBlock(generator, depth):\n",
    "    for param in generator.gen_block[0].parameters():\n",
    "        param.requires_grad = False\n",
    "    generator.gen_block.insert(0, GenGeneralConvBlock(nf(depth), nf(depth+1), False))\n",
    "    \"\"\"i = 0\n",
    "    for block in model.gen_block:\n",
    "        print(f\"Block {i} : \")\n",
    "        i += 1\n",
    "        for param in block.parameters():\n",
    "            print(f\"Requires grad : {param.requires_grad}\")\n",
    "        print(\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(Module):\n",
    "    def __init__(self, latent_size=512, depth=7, use_eql=False):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.dis_block = ModuleList([DisFinalBlock(nf(1), latent_size, use_eql)])\n",
    "\n",
    "        ConvBlock = EqualizedConv2d if use_eql else Conv2d\n",
    "\n",
    "        self.from_rgb = ModuleList(\n",
    "            reversed(\n",
    "                [\n",
    "                    ConvBlock(3, latent_size, kernel_size=(1, 1))\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.from_rgb[0](x)\n",
    "        for layer in self.dis_block:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Discriminator Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def addDisBlock(discriminator, depth):\n",
    "    for param in discriminator.dis_block[0].parameters():\n",
    "        param.requires_grad = False\n",
    "    discriminator.dis_block.insert(0, DisGeneralConvBlock(nf(depth), nf(depth+1), False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DCGAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, noise_fn, dataloader, device, batch_size=20, lr_g=1e-3, lr_d=2e-4, depth=7):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.generator = Generator(1, 3, False).to(device)\n",
    "        self.discriminator = Discriminator().to(device)\n",
    "        self.noise_fn = noise_fn\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.criterion = BCEWithLogitsLoss()\n",
    "        self.g_optimizer = Adam(self.generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "        self.d_optimizer = Adam(self.discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "\n",
    "        self.target_ones = torch.ones(batch_size, 1)\n",
    "        self.target_zeros = torch.zeros(batch_size, 1)\n",
    "\n",
    "    def create_sample(self):\n",
    "        latent_vec = self.noise_fn(self.batch_size)\n",
    "        with torch.no_grad():\n",
    "            samples = self.generator(latent_vec)\n",
    "        samples = samples.cpu()\n",
    "        return samples\n",
    "\n",
    "    def train_step_generator(self):\n",
    "        self.generator.zero_grad()\n",
    "        latent_vec = self.noise_fn(self.batch_size)\n",
    "        fake_sample = self.generator(latent_vec)\n",
    "        prediction = self.discriminator(fake_sample)\n",
    "        loss = self.criterion(prediction, self.target_ones)\n",
    "        loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def train_step_discriminator(self, real_sample):\n",
    "        #self.discriminator.zero_grad()\n",
    "        pred_real = self.discriminator(real_sample)\n",
    "        loss_real = self.criterion(pred_real, self.target_ones)\n",
    "        latent_vec = self.noise_fn(self.batch_size)\n",
    "        with torch.no_grad():\n",
    "            fake_sample = self.generator(latent_vec)\n",
    "        fake_pred = self.discriminator(fake_sample)\n",
    "        loss_fake = self.criterion(fake_pred, self.target_zeros)\n",
    "\n",
    "        loss = (loss_real + loss_fake) / 2\n",
    "        self.d_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        return loss_real.item(), loss_fake.item()\n",
    "\n",
    "    def train_gan(self, stride, kernel_size, epoch_train_d):\n",
    "        loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
    "        array_loss = [[], [], []]\n",
    "        for batch_i, (image, _) in enumerate(self.dataloader):\n",
    "            image = split_image(image, stride, kernel_size)\n",
    "            dataloader_split = torch.utils.data.DataLoader(image, batch_size=batch_size,\n",
    "                                                           shuffle=True)\n",
    "            for batch_si, image in enumerate(dataloader_split):\n",
    "                image = image.to(self.device)\n",
    "                for i in range(epoch_train_d):\n",
    "                    ldr, ldf = self.train_step_discriminator(image)\n",
    "                    loss_d_fake_running += ldf\n",
    "                    loss_d_real_running += ldr\n",
    "                loss_g_running += self.train_step_generator()\n",
    "                break\n",
    "            break\n",
    "            if batch_i % 100 == 99:\n",
    "                array_loss[0].append(loss_g_running/((batch_i+1)*(batch_si+1)))\n",
    "                array_loss[1].append(loss_d_real_running/((batch_i+1)*((batch_si+1)*epoch_train_d)))\n",
    "                array_loss[2].append(loss_d_fake_running/((batch_i+1)*((batch_si+1)*epoch_train_d)))\n",
    "        return np.array(array_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataloader, ngpu=0, path=\"./\"):\n",
    "    batch_size = 20\n",
    "    epochs = 1\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "    latent_size = 4\n",
    "    noise_fn = lambda x: torch.randn(x, 64, 2, 2)\n",
    "    gan = DCGAN(noise_fn=noise_fn, dataloader=dataloader, device=device, batch_size=batch_size)\n",
    "    tab_loss = []\n",
    "    for i in range(epochs):\n",
    "        loss = (gan.train_gan(2, 4, 5))\n",
    "        tab_loss.append(loss)\n",
    "    gan.generator.to(\"cpu\")\n",
    "    gan.discriminator.to(\"cpu\")\n",
    "    torch.save({\n",
    "        \"generator\":gan.generator.state_dict(),\n",
    "        \"discriminator\":gan.discriminator.state_dict()\n",
    "    }, (path +\"gen_disc.pth\"))\n",
    "    save_loss(tab_loss)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = main(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_loss(loss):\n",
    "    fix, axe = plt.subplots(figsize=(12, 8))\n",
    "    plt.plot(loss[0], label=\"Generator\", alpha=0.5)\n",
    "    plt.plot(loss[1], label=\"Discriminator fake\", alpha=0.8)\n",
    "    plt.plot(loss[2],  label=\"Discriminator real\")\n",
    "    plt.title(\"Training loss\")\n",
    "    axe.set_ylabel(\"Loss\")\n",
    "    axe.set_xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "display_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
